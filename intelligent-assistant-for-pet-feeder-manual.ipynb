{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":110281,"databundleVersionId":13238728,"sourceType":"competition"},{"sourceId":12754753,"sourceType":"datasetVersion","datasetId":8063089}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install PyPDF2 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:12.462409Z","iopub.execute_input":"2025-08-13T17:04:12.463216Z","iopub.status.idle":"2025-08-13T17:04:16.216707Z","shell.execute_reply.started":"2025-08-13T17:04:12.463186Z","shell.execute_reply":"2025-08-13T17:04:16.215436Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\npio.renderers.default = 'iframe'\n\nfrom google.cloud import bigquery\nfrom google.cloud.exceptions import NotFound, Forbidden\nimport google.auth.exceptions\n\nimport logging\nimport warnings\nfrom typing import Dict, List, Optional, Tuple, Union, Any\nfrom dataclasses import dataclass\nimport json\nimport time\nfrom pathlib import Path\nimport re\nfrom datetime import datetime\n\nimport PyPDF2\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nwarnings.filterwarnings('ignore')\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.218687Z","iopub.execute_input":"2025-08-13T17:04:16.219084Z","iopub.status.idle":"2025-08-13T17:04:16.228756Z","shell.execute_reply.started":"2025-08-13T17:04:16.219048Z","shell.execute_reply":"2025-08-13T17:04:16.227635Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Configuration Management\n\n@dataclass\nclass Config:\n    \"\"\"System configuration loaded from environment variables\"\"\"\n    PROJECT_ID: str = os.getenv('GCP_PROJECT_ID', 'gemma-vision-462110')\n    DATASET_ID: str = os.getenv('BQ_DATASET_ID', 'semantic_detective_v2')\n    EMBEDDING_MODEL: str = os.getenv('EMBEDDING_MODEL', 'text-embedding-004')\n    \n    CHUNK_SIZE: int = int(os.getenv('CHUNK_SIZE', '500'))\n    CHUNK_OVERLAP: int = int(os.getenv('CHUNK_OVERLAP', '100'))\n    MIN_CHUNK_WORDS: int = int(os.getenv('MIN_CHUNK_WORDS', '20'))\n    \n    MAX_FEATURES: int = int(os.getenv('MAX_FEATURES', '3000'))\n    TOP_K_RESULTS: int = int(os.getenv('TOP_K_RESULTS', '10'))\n    SIMILARITY_THRESHOLD: float = float(os.getenv('SIMILARITY_THRESHOLD', '0.1'))  # Much lower threshold\n    TECHNICAL_BOOST_FACTOR: float = float(os.getenv('TECHNICAL_BOOST', '0.15'))\n    \n    BATCH_SIZE: int = int(os.getenv('BATCH_SIZE', '1000'))\n    MAX_RETRIES: int = int(os.getenv('MAX_RETRIES', '3'))\n    TIMEOUT_SECONDS: int = int(os.getenv('TIMEOUT_SECONDS', '300'))\n    \n    ENVIRONMENT: str = os.getenv('ENVIRONMENT', 'development')\n    LOG_LEVEL: str = os.getenv('LOG_LEVEL', 'INFO')\n    \n    def get_full_table_id(self, table_name: str) -> str:\n        return f\"{self.PROJECT_ID}.{self.DATASET_ID}.{table_name}\"\n    \n    def is_production(self) -> bool:\n        return self.ENVIRONMENT.lower() == 'production'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.229923Z","iopub.execute_input":"2025-08-13T17:04:16.230256Z","iopub.status.idle":"2025-08-13T17:04:16.260031Z","shell.execute_reply.started":"2025-08-13T17:04:16.230226Z","shell.execute_reply":"2025-08-13T17:04:16.259046Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# SQL Query Templates\n\nclass SQLTemplates:\n    \"\"\"BigQuery ML SQL query templates for semantic search operations\"\"\"\n    \n    @staticmethod\n    def create_embedding_table(project_id: str, dataset_id: str, source_table: str, \n                              target_table: str, embedding_model: str, min_words: int) -> str:\n        \"\"\"Generate embeddings using ML.GENERATE_EMBEDDING\"\"\"\n        return f\"\"\"\n        CREATE OR REPLACE TABLE `{project_id}.{dataset_id}.{target_table}` AS\n        SELECT \n            document_id,\n            page_number,\n            section_number,\n            text_content,\n            content_category,\n            word_count,\n            char_count,\n            has_technical_terms,\n            ML.GENERATE_EMBEDDING(\n                MODEL `{embedding_model}`,\n                content => text_content,\n                task_type => 'SEMANTIC_SIMILARITY'\n            ) AS text_embedding,\n            CURRENT_DATETIME() as embedding_created_at\n        FROM `{project_id}.{dataset_id}.{source_table}`\n        WHERE LENGTH(text_content) > {min_words}\n            AND text_content IS NOT NULL\n        \"\"\"\n    \n    @staticmethod\n    def create_vector_index(project_id: str, dataset_id: str, table_name: str, \n                           index_name: str) -> str:\n        \"\"\"Create vector index for faster similarity search\"\"\"\n        return f\"\"\"\n        CREATE VECTOR INDEX `{index_name}`\n        ON `{project_id}.{dataset_id}.{table_name}`(text_embedding)\n        OPTIONS(\n            index_type='IVF',\n            distance_type='COSINE',\n            ivf_options='{{\"num_lists\": 1000}}'\n        )\n        \"\"\"\n    \n    @staticmethod\n    def vector_search_query(project_id: str, dataset_id: str, embedding_table: str,\n                           embedding_model: str, top_k: int, boost_factor: float) -> str:\n        \"\"\"Execute semantic search using VECTOR_SEARCH\"\"\"\n        return f\"\"\"\n        WITH query_embedding AS (\n            SELECT ML.GENERATE_EMBEDDING(\n                MODEL `{embedding_model}`,\n                content => @search_query,\n                task_type => 'SEMANTIC_SIMILARITY'\n            ) AS query_vector\n        ),\n        search_results AS (\n            SELECT \n                base_rowid,\n                distance\n            FROM VECTOR_SEARCH(\n                TABLE `{project_id}.{dataset_id}.{embedding_table}`,\n                'text_embedding',\n                (SELECT query_vector FROM query_embedding),\n                top_k => {top_k},\n                distance_type => 'COSINE'\n            )\n        )\n        SELECT \n            base.document_id,\n            base.page_number,\n            base.section_number,\n            base.text_content,\n            base.content_category,\n            base.word_count,\n            base.has_technical_terms,\n            search_results.distance AS similarity_distance,\n            (1.0 - search_results.distance) AS similarity_score,\n            CASE \n                WHEN base.has_technical_terms THEN \n                    (1.0 - search_results.distance) + {boost_factor}\n                ELSE \n                    (1.0 - search_results.distance)\n            END AS boosted_score,\n            RANK() OVER (ORDER BY \n                CASE \n                    WHEN base.has_technical_terms THEN \n                        (1.0 - search_results.distance) + {boost_factor}\n                    ELSE \n                        (1.0 - search_results.distance)\n                END DESC\n            ) AS search_rank\n        FROM search_results\n        JOIN `{project_id}.{dataset_id}.{embedding_table}` AS base\n        ON search_results.base_rowid = base.rowid\n        WHERE (1.0 - search_results.distance) >= 0.3  -- Similarity threshold\n        ORDER BY boosted_score DESC\n        \"\"\"\n    \n    @staticmethod\n    def similarity_analysis_query(project_id: str, dataset_id: str, \n                                 embedding_table: str) -> str:\n        \"\"\"Analyze document similarity patterns\"\"\"\n        return f\"\"\"\n        WITH document_similarities AS (\n            SELECT \n                a.document_id as doc_a,\n                b.document_id as doc_b,\n                a.content_category as category_a,\n                b.content_category as category_b,\n                ML.DISTANCE(a.text_embedding, b.text_embedding, 'COSINE') as cosine_distance,\n                (1.0 - ML.DISTANCE(a.text_embedding, b.text_embedding, 'COSINE')) as similarity_score\n            FROM `{project_id}.{dataset_id}.{embedding_table}` a\n            CROSS JOIN `{project_id}.{dataset_id}.{embedding_table}` b\n            WHERE a.document_id != b.document_id\n        )\n        SELECT \n            category_a,\n            category_b,\n            COUNT(*) as pair_count,\n            AVG(similarity_score) as avg_similarity,\n            MAX(similarity_score) as max_similarity,\n            MIN(similarity_score) as min_similarity\n        FROM document_similarities\n        WHERE similarity_score > 0.5\n        GROUP BY category_a, category_b\n        ORDER BY avg_similarity DESC\n        \"\"\"\n    \n    @staticmethod\n    def content_insights_query(project_id: str, dataset_id: str, \n                              chunks_table: str) -> str:\n        \"\"\"Generate content insights and statistics\"\"\"\n        return f\"\"\"\n        WITH content_stats AS (\n            SELECT \n                content_category,\n                has_technical_terms,\n                COUNT(*) as section_count,\n                AVG(word_count) as avg_words,\n                AVG(char_count) as avg_chars,\n                MIN(word_count) as min_words,\n                MAX(word_count) as max_words\n            FROM `{project_id}.{dataset_id}.{chunks_table}`\n            GROUP BY content_category, has_technical_terms\n        ),\n        category_totals AS (\n            SELECT \n                content_category,\n                SUM(section_count) as total_sections,\n                SUM(CASE WHEN has_technical_terms THEN section_count ELSE 0 END) as technical_sections\n            FROM content_stats\n            GROUP BY content_category\n        )\n        SELECT \n            ct.content_category,\n            ct.total_sections,\n            ct.technical_sections,\n            ROUND(ct.technical_sections / ct.total_sections * 100, 2) as technical_percentage,\n            ROUND(AVG(cs.avg_words), 1) as avg_words_per_section,\n            ROUND(AVG(cs.avg_chars), 1) as avg_chars_per_section\n        FROM category_totals ct\n        JOIN content_stats cs ON ct.content_category = cs.content_category\n        GROUP BY ct.content_category, ct.total_sections, ct.technical_sections\n        ORDER BY ct.total_sections DESC\n        \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.262235Z","iopub.execute_input":"2025-08-13T17:04:16.262528Z","iopub.status.idle":"2025-08-13T17:04:16.284325Z","shell.execute_reply.started":"2025-08-13T17:04:16.262503Z","shell.execute_reply":"2025-08-13T17:04:16.283424Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# BigQuery ML Integration\n\nclass BigQueryMLConnector:\n    \"\"\"Handles BigQuery ML operations with proper error handling and feature detection\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.client = None\n        self.is_connected = False\n        self.ml_features = {\n            'generate_embedding': False,\n            'vector_search': False,\n            'create_index': False\n        }\n    \n    def connect(self) -> bool:\n        \"\"\"Establish BigQuery connection and test ML capabilities\"\"\"\n        try:\n            self.client = bigquery.Client(project=self.config.PROJECT_ID)\n            \n            # Test basic connectivity\n            test_query = \"SELECT CURRENT_DATETIME() as timestamp\"\n            self.client.query(test_query).result()\n            self.is_connected = True\n            \n            # Test ML capabilities\n            self._probe_ml_features()\n            return True\n            \n        except google.auth.exceptions.DefaultCredentialsError:\n            logger.warning(\"BigQuery credentials unavailable\")\n            return False\n        except Exception as e:\n            logger.warning(f\"BigQuery connection failed: {e}\")\n            return False\n    \n    def _probe_ml_features(self) -> None:\n        \"\"\"Test availability of ML functions\"\"\"\n        if not self.is_connected:\n            return\n        \n        try:\n            # Test embedding generation\n            embedding_test = \"\"\"\n            SELECT ML.GENERATE_EMBEDDING(\n                MODEL `text-embedding-004`,\n                content => 'test'\n            ) as embedding_test\n            \"\"\"\n            self.client.query(embedding_test).result()\n            self.ml_features['generate_embedding'] = True\n            self.ml_features['vector_search'] = True\n            self.ml_features['create_index'] = True\n            \n        except Exception:\n            pass  # ML features not available\n    \n    def setup_workspace(self) -> bool:\n        \"\"\"Create dataset and prepare workspace\"\"\"\n        if not self.is_connected:\n            return False\n        \n        try:\n            self.client.get_dataset(self.config.DATASET_ID)\n        except NotFound:\n            dataset = bigquery.Dataset(f\"{self.config.PROJECT_ID}.{self.config.DATASET_ID}\")\n            dataset.location = \"US\"\n            self.client.create_dataset(dataset, timeout=30)\n        \n        return True\n    \n    def upload_documents(self, df: pd.DataFrame, table_name: str) -> bool:\n        \"\"\"Upload document chunks to BigQuery\"\"\"\n        if not self.is_connected:\n            return False\n        \n        try:\n            table_id = self.config.get_full_table_id(table_name)\n            job_config = bigquery.LoadJobConfig(\n                write_disposition=\"WRITE_TRUNCATE\",\n                autodetect=True\n            )\n            \n            job = self.client.load_table_from_dataframe(df, table_id, job_config=job_config)\n            job.result(timeout=self.config.TIMEOUT_SECONDS)\n            \n            logger.info(f\"Uploaded {len(df)} rows to {table_id}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Upload failed: {e}\")\n            return False\n    \n    def generate_embeddings(self, source_table: str, target_table: str) -> bool:\n        \"\"\"Generate embeddings using BigQuery ML\"\"\"\n        if not self.ml_features['generate_embedding']:\n            return False\n        \n        try:\n            query = SQLTemplates.create_embedding_table(\n                self.config.PROJECT_ID,\n                self.config.DATASET_ID,\n                source_table,\n                target_table,\n                self.config.EMBEDDING_MODEL,\n                self.config.MIN_CHUNK_WORDS\n            )\n            \n            job = self.client.query(query)\n            job.result(timeout=self.config.TIMEOUT_SECONDS)\n            \n            logger.info(f\"Generated embeddings for {self.config.get_full_table_id(target_table)}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Embedding generation failed: {e}\")\n            return False\n    \n    def create_vector_index(self, table_name: str, index_name: str = \"semantic_search_idx\") -> bool:\n        \"\"\"Create vector index for faster search on large datasets\"\"\"\n        if not self.ml_features['create_index']:\n            return False\n        \n        try:\n            query = SQLTemplates.create_vector_index(\n                self.config.PROJECT_ID,\n                self.config.DATASET_ID,\n                table_name,\n                index_name\n            )\n            \n            job = self.client.query(query)\n            job.result(timeout=self.config.TIMEOUT_SECONDS)\n            logger.info(f\"Vector index {index_name} created successfully\")\n            return True\n            \n        except Exception as e:\n            logger.warning(f\"Vector index creation failed: {e}\")\n            return False\n    \n    def vector_search(self, query: str, embedding_table: str, top_k: int = None) -> pd.DataFrame:\n        \"\"\"Execute vector search using BigQuery\"\"\"\n        if not self.ml_features['vector_search']:\n            raise ValueError(\"Vector search not available\")\n        \n        if top_k is None:\n            top_k = self.config.TOP_K_RESULTS\n        \n        search_query = SQLTemplates.vector_search_query(\n            self.config.PROJECT_ID,\n            self.config.DATASET_ID,\n            embedding_table,\n            self.config.EMBEDDING_MODEL,\n            top_k,\n            self.config.TECHNICAL_BOOST_FACTOR\n        )\n        \n        job_config = bigquery.QueryJobConfig(\n            query_parameters=[\n                bigquery.ScalarQueryParameter(\"search_query\", \"STRING\", query)\n            ]\n        )\n        \n        result_df = self.client.query(search_query, job_config=job_config).to_dataframe()\n        logger.info(f\"Vector search completed: {len(result_df)} results for '{query}'\")\n        \n        return result_df\n    \n    def analyze_content_insights(self, chunks_table: str) -> pd.DataFrame:\n        \"\"\"Generate content analysis insights using SQL\"\"\"\n        if not self.is_connected:\n            return pd.DataFrame()\n        \n        try:\n            query = SQLTemplates.content_insights_query(\n                self.config.PROJECT_ID,\n                self.config.DATASET_ID,\n                chunks_table\n            )\n            \n            return self.client.query(query).to_dataframe()\n            \n        except Exception as e:\n            logger.error(f\"Content insights analysis failed: {e}\")\n            return pd.DataFrame()\n    \n    def analyze_similarity_patterns(self, embedding_table: str) -> pd.DataFrame:\n        \"\"\"Analyze document similarity patterns across categories\"\"\"\n        if not self.is_connected:\n            return pd.DataFrame()\n        \n        try:\n            query = SQLTemplates.similarity_analysis_query(\n                self.config.PROJECT_ID,\n                self.config.DATASET_ID,\n                embedding_table\n            )\n            \n            return self.client.query(query).to_dataframe()\n            \n        except Exception as e:\n            logger.error(f\"Similarity analysis failed: {e}\")\n            return pd.DataFrame()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.285307Z","iopub.execute_input":"2025-08-13T17:04:16.285644Z","iopub.status.idle":"2025-08-13T17:04:16.310745Z","shell.execute_reply.started":"2025-08-13T17:04:16.285614Z","shell.execute_reply":"2025-08-13T17:04:16.309867Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Document Processing\n\nclass DocumentProcessor:\n    \"\"\"Extracts and processes unstructured document content\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.technical_keywords = [\n            'troubleshoot', 'problem', 'error', 'fix', 'maintenance', 'repair',\n            'battery', 'power', 'program', 'schedule', 'setup', 'install',\n            'configure', 'settings', 'manual', 'instructions', 'procedure',\n            'warning', 'caution', 'safety', 'support', 'help', 'guide'\n        ]\n    \n    def process_pdf(self, pdf_path: str) -> pd.DataFrame:\n        \"\"\"Extract and structure PDF content\"\"\"\n        if not Path(pdf_path).exists():\n            logger.error(f\"File not found: {pdf_path}\")\n            return self._generate_sample_data()\n        \n        try:\n            return self._extract_pdf_content(pdf_path)\n        except Exception as e:\n            logger.error(f\"PDF processing failed: {e}\")\n            return self._generate_sample_data()\n    \n    def _extract_pdf_content(self, pdf_path: str) -> pd.DataFrame:\n        \"\"\"Extract content using PyPDF2\"\"\"\n        data = []\n        \n        with open(pdf_path, 'rb') as file:\n            reader = PyPDF2.PdfReader(file)\n            \n            for page_num, page in enumerate(reader.pages):\n                text = page.extract_text()\n                sections = self._create_sections(text)\n                \n                for section_num, section in enumerate(sections):\n                    if len(section.split()) >= self.config.MIN_CHUNK_WORDS:\n                        data.append({\n                            'document_id': f\"doc_1_p{page_num+1}_s{section_num+1}\",\n                            'page_number': page_num + 1,\n                            'section_number': section_num + 1,\n                            'content_type': 'text',\n                            'text_content': section.strip(),\n                            'word_count': len(section.split()),\n                            'char_count': len(section),\n                            'has_technical_terms': self._is_technical(section),\n                            'content_category': self._categorize(section),\n                            'extraction_timestamp': pd.Timestamp.now().isoformat()\n                        })\n        \n        return pd.DataFrame(data)\n    \n    def _create_sections(self, text: str) -> List[str]:\n        \"\"\"Split text into semantically meaningful sections\"\"\"\n        sections = []\n        \n        # Split by paragraph breaks\n        paragraphs = re.split(r'\\n\\n+', text)\n        \n        for paragraph in paragraphs:\n            paragraph = paragraph.strip()\n            if not paragraph:\n                continue\n            \n            # Split long paragraphs by sentences\n            if len(paragraph.split()) > self.config.CHUNK_SIZE:\n                sentences = re.split(r'[.!?]+', paragraph)\n                current_section = \"\"\n                \n                for sentence in sentences:\n                    sentence = sentence.strip()\n                    if not sentence:\n                        continue\n                    \n                    test_section = current_section + \" \" + sentence + \".\"\n                    if len(test_section.split()) <= self.config.CHUNK_SIZE:\n                        current_section = test_section\n                    else:\n                        if current_section.strip():\n                            sections.append(current_section.strip())\n                        current_section = sentence + \".\"\n                \n                if current_section.strip():\n                    sections.append(current_section.strip())\n            else:\n                sections.append(paragraph)\n        \n        return [s for s in sections if len(s.split()) >= self.config.MIN_CHUNK_WORDS]\n    \n    def _is_technical(self, text: str) -> bool:\n        \"\"\"Detect technical content using keyword matching\"\"\"\n        text_lower = text.lower()\n        return any(keyword in text_lower for keyword in self.technical_keywords)\n    \n    def _categorize(self, text: str) -> str:\n        \"\"\"Categorize content by topic\"\"\"\n        text_lower = text.lower()\n        \n        if any(word in text_lower for word in ['troubleshoot', 'problem', 'error', 'fix']):\n            return 'troubleshooting'\n        elif any(word in text_lower for word in ['program', 'schedule', 'setup', 'configure']):\n            return 'programming'\n        elif any(word in text_lower for word in ['maintenance', 'clean', 'care', 'replace']):\n            return 'maintenance'\n        elif any(word in text_lower for word in ['battery', 'power', 'adapter', 'electrical']):\n            return 'power'\n        elif any(word in text_lower for word in ['safety', 'warning', 'caution', 'danger']):\n            return 'safety'\n        elif any(word in text_lower for word in ['install', 'setup', 'initial', 'first']):\n            return 'installation'\n        else:\n            return 'general'\n    \n    def _generate_sample_data(self) -> pd.DataFrame:\n        \"\"\"Generate sample data for demonstration purposes\"\"\"\n        sample_data = [\n            {\n                'document_id': 'doc_1_p1_s1',\n                'page_number': 1,\n                'section_number': 1,\n                'content_type': 'text',\n                'text_content': 'CritterCuisine Pro 5000 Automatic Pet Feeder User Manual Version 1.2. This advanced automatic pet feeder is designed to provide precise, reliable, and customizable feeding for your beloved pets. Features include programmable schedules, portion control, and voice recording capabilities.',\n                'word_count': 38,\n                'char_count': 260,\n                'has_technical_terms': True,\n                'content_category': 'general',\n                'extraction_timestamp': pd.Timestamp.now().isoformat()\n            },\n            {\n                'document_id': 'doc_1_p2_s1',\n                'page_number': 2,\n                'section_number': 1,\n                'content_type': 'text',\n                'text_content': 'Troubleshooting Common Issues: If the feeder is not dispensing food correctly, first check that the food hopper is properly installed and contains food. Verify that the dispensing mechanism is not blocked by debris or oversized food pieces. Check the power connection and ensure the unit is receiving electricity.',\n                'word_count': 45,\n                'char_count': 320,\n                'has_technical_terms': True,\n                'content_category': 'troubleshooting',\n                'extraction_timestamp': pd.Timestamp.now().isoformat()\n            },\n            {\n                'document_id': 'doc_1_p3_s1',\n                'page_number': 3,\n                'section_number': 1,\n                'content_type': 'text',\n                'text_content': 'Programming Feeding Schedules: To set up automatic feeding times, press and hold the PROGRAM button for 3 seconds until the display shows \"PROG\". Use the UP and DOWN arrow buttons to set the hour, then press SET. Repeat this process to set minutes and portion size. You can program up to 6 meals per day.',\n                'word_count': 52,\n                'char_count': 340,\n                'has_technical_terms': True,\n                'content_category': 'programming',\n                'extraction_timestamp': pd.Timestamp.now().isoformat()\n            }\n        ]\n        \n        return pd.DataFrame(sample_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.311729Z","iopub.execute_input":"2025-08-13T17:04:16.311964Z","iopub.status.idle":"2025-08-13T17:04:16.334207Z","shell.execute_reply.started":"2025-08-13T17:04:16.311946Z","shell.execute_reply":"2025-08-13T17:04:16.333080Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Local Embedding Engine\n\n\nclass LocalEmbeddingEngine:\n    \"\"\"TF-IDF based local embedding system for fallback scenarios\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.vectorizer = None\n        self.embeddings = None\n        self.feature_names = None\n        self.is_trained = False\n    \n    def create_embeddings(self, documents_df: pd.DataFrame) -> bool:\n        \"\"\"Generate TF-IDF embeddings for local search\"\"\"\n        try:\n            self.vectorizer = TfidfVectorizer(\n                max_features=self.config.MAX_FEATURES,\n                stop_words='english',\n                ngram_range=(1, 2),  # Reduced from (1,3) for better matches\n                min_df=1,\n                max_df=0.85,  # More lenient\n                sublinear_tf=True,\n                analyzer='word',\n                lowercase=True,\n                token_pattern=r'\\b[a-zA-Z][a-zA-Z0-9]{2,}\\b'\n            )\n            \n            text_corpus = documents_df['text_content'].fillna('')\n            self.embeddings = self.vectorizer.fit_transform(text_corpus)\n            self.feature_names = self.vectorizer.get_feature_names_out()\n            self.is_trained = True\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Local embedding creation failed: {e}\")\n            return False\n    \n    def search(self, query: str, documents_df: pd.DataFrame, top_k: int = None) -> pd.DataFrame:\n        \"\"\"Execute semantic search using local embeddings\"\"\"\n        if not self.is_trained:\n            raise ValueError(\"Embeddings not created. Call create_embeddings() first.\")\n        \n        if top_k is None:\n            top_k = self.config.TOP_K_RESULTS\n        \n        try:\n            # Generate query embedding\n            query_embedding = self.vectorizer.transform([query])\n            \n            # Calculate similarities\n            similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n            \n            # Apply technical boost\n            technical_boost = documents_df['has_technical_terms'].values.astype(float) * self.config.TECHNICAL_BOOST_FACTOR\n            boosted_similarities = similarities + technical_boost\n            \n            # Get top results\n            top_indices = np.argsort(boosted_similarities)[::-1][:top_k]\n            \n            # Create results DataFrame\n            results = documents_df.iloc[top_indices].copy()\n            results['similarity_score'] = boosted_similarities[top_indices]\n            results['base_similarity'] = similarities[top_indices]\n            results['technical_boost'] = technical_boost[top_indices]\n            results['rank'] = range(1, len(results) + 1)\n            \n            # Calculate word overlap\n            query_words = set(query.lower().split())\n            results['word_overlap'] = results['text_content'].apply(\n                lambda text: len(query_words.intersection(set(text.lower().split()))) / max(len(query_words), 1)\n            )\n            \n            # Filter by threshold\n            results = results[results['similarity_score'] >= self.config.SIMILARITY_THRESHOLD]\n            \n            return results[['document_id', 'page_number', 'section_number', 'text_content', \n                           'content_category', 'similarity_score', 'word_overlap', 'rank',\n                           'has_technical_terms', 'word_count']]\n            \n        except Exception as e:\n            logger.error(f\"Local search failed: {e}\")\n            return pd.DataFrame()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.335381Z","iopub.execute_input":"2025-08-13T17:04:16.335721Z","iopub.status.idle":"2025-08-13T17:04:16.360914Z","shell.execute_reply.started":"2025-08-13T17:04:16.335691Z","shell.execute_reply":"2025-08-13T17:04:16.359719Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Main System Class\n\n\nclass SemanticDetective:\n    \"\"\"Main system orchestrating document processing and semantic search\"\"\"\n    \n    def __init__(self, config: Config = None):\n        self.config = config or Config()\n        self.bq_connector = BigQueryMLConnector(self.config)\n        self.doc_processor = DocumentProcessor(self.config)\n        self.local_embedder = LocalEmbeddingEngine(self.config)\n        \n        self.documents_df = None\n        self.is_bigquery_mode = False\n        self.is_local_mode = False\n        self.embeddings_ready = False\n    \n    def connect_to_bigquery(self) -> bool:\n        \"\"\"Establish BigQuery connection\"\"\"\n        connected = self.bq_connector.connect()\n        if connected:\n            self.bq_connector.setup_workspace()\n        return connected\n    \n    def load_documents(self, pdf_path: str) -> bool:\n        \"\"\"Load and process documents\"\"\"\n        self.documents_df = self.doc_processor.process_pdf(pdf_path)\n        \n        if len(self.documents_df) > 0:\n            print(f\"Loaded {len(self.documents_df)} document sections from {self.documents_df['page_number'].nunique()} pages\")\n            \n            # Show sample of what was processed\n            sample_data = self.documents_df[['page_number', 'content_category', 'word_count', 'has_technical_terms']].head()\n            print(sample_data.to_string(index=False))\n            \n        return len(self.documents_df) > 0\n    \n    def initialize_search(self) -> bool:\n        \"\"\"Initialize search capabilities\"\"\"\n        if self.documents_df is None:\n            return False\n        \n        # Try BigQuery ML first\n        if self.bq_connector.is_connected and self.bq_connector.ml_features['generate_embedding']:\n            if self._setup_bigquery_search():\n                self.is_bigquery_mode = True\n                self.embeddings_ready = True\n                print(\"Initialized BigQuery ML vector search\")\n                return True\n        \n        # Fallback to local search\n        if self.local_embedder.create_embeddings(self.documents_df):\n            self.is_local_mode = True\n            self.embeddings_ready = True\n            print(f\"Initialized local embeddings: {self.local_embedder.embeddings.shape}\")\n            return True\n        \n        return False\n    \n    def _setup_bigquery_search(self) -> bool:\n        \"\"\"Setup BigQuery ML search pipeline\"\"\"\n        try:\n            # Upload documents\n            if not self.bq_connector.upload_documents(self.documents_df, 'document_chunks'):\n                return False\n            \n            # Generate embeddings\n            if not self.bq_connector.generate_embeddings('document_chunks', 'document_embeddings'):\n                return False\n            \n            # Create vector index for large datasets (optional)\n            if len(self.documents_df) > 1000:\n                self.bq_connector.create_vector_index('document_embeddings')\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"BigQuery setup failed: {e}\")\n            return False\n    \n    def get_content_insights(self) -> pd.DataFrame:\n        \"\"\"Get detailed content analysis using BigQuery SQL\"\"\"\n        if self.is_bigquery_mode:\n            return self.bq_connector.analyze_content_insights('document_chunks')\n        else:\n            # Fallback to pandas analysis\n            if self.documents_df is None:\n                return pd.DataFrame()\n            \n            insights = self.documents_df.groupby(['content_category', 'has_technical_terms']).agg({\n                'document_id': 'count',\n                'word_count': ['mean', 'min', 'max'],\n                'char_count': 'mean'\n            }).round(2)\n            \n            return insights\n    \n    def get_similarity_analysis(self) -> pd.DataFrame:\n        \"\"\"Analyze similarity patterns between document categories\"\"\"\n        if self.is_bigquery_mode:\n            return self.bq_connector.analyze_similarity_patterns('document_embeddings')\n        else:\n            # Local similarity analysis using embeddings\n            if not self.is_local_mode or self.local_embedder.embeddings is None:\n                return pd.DataFrame()\n            \n            similarities = cosine_similarity(self.local_embedder.embeddings)\n            \n            # Create category-based similarity analysis\n            categories = self.documents_df['content_category'].values\n            unique_cats = sorted(self.documents_df['content_category'].unique())\n            \n            results = []\n            for i, cat_a in enumerate(unique_cats):\n                for j, cat_b in enumerate(unique_cats):\n                    if i <= j:  # Avoid duplicates\n                        mask_a = categories == cat_a\n                        mask_b = categories == cat_b\n                        \n                        if cat_a == cat_b:\n                            # Within-category similarities (exclude self-similarity)\n                            cat_similarities = similarities[mask_a][:, mask_b]\n                            np.fill_diagonal(cat_similarities, np.nan)\n                            avg_sim = np.nanmean(cat_similarities)\n                        else:\n                            # Between-category similarities\n                            avg_sim = np.mean(similarities[mask_a][:, mask_b])\n                        \n                        results.append({\n                            'category_a': cat_a,\n                            'category_b': cat_b,\n                            'avg_similarity': avg_sim,\n                            'pair_count': np.sum(mask_a) * np.sum(mask_b)\n                        })\n            \n            return pd.DataFrame(results)\n    \n    def search(self, query: str, top_k: int = None) -> pd.DataFrame:\n        \"\"\"Execute semantic search\"\"\"\n        if not self.embeddings_ready:\n            raise ValueError(\"Search not initialized. Call initialize_search() first.\")\n        \n        if self.is_bigquery_mode:\n            return self.bq_connector.vector_search(query, 'document_embeddings', top_k)\n        elif self.is_local_mode:\n            return self.local_embedder.search(query, self.documents_df, top_k)\n        else:\n            return pd.DataFrame()\n    \n    def get_sql_queries(self) -> Dict[str, str]:\n        \"\"\"Return the SQL queries used in the system for documentation\"\"\"\n        return {\n            'embedding_generation': SQLTemplates.create_embedding_table(\n                self.config.PROJECT_ID, self.config.DATASET_ID, \n                'source_table', 'target_table', \n                self.config.EMBEDDING_MODEL, self.config.MIN_CHUNK_WORDS\n            ),\n            'vector_search': SQLTemplates.vector_search_query(\n                self.config.PROJECT_ID, self.config.DATASET_ID,\n                'embedding_table', self.config.EMBEDDING_MODEL,\n                self.config.TOP_K_RESULTS, self.config.TECHNICAL_BOOST_FACTOR\n            ),\n            'vector_index': SQLTemplates.create_vector_index(\n                self.config.PROJECT_ID, self.config.DATASET_ID,\n                'embedding_table', 'search_index'\n            ),\n            'content_insights': SQLTemplates.content_insights_query(\n                self.config.PROJECT_ID, self.config.DATASET_ID, 'chunks_table'\n            )\n        }\n    \n    def create_visualizations(self) -> Dict[str, go.Figure]:\n        \"\"\"Generate analysis visualizations\"\"\"\n        if self.documents_df is None:\n            return {}\n        \n        figures = {}\n        \n        # Content overview\n        fig = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=('Content by Page', 'Technical Content', \n                          'Word Distribution', 'Categories'),\n            specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                   [{\"type\": \"histogram\"}, {\"type\": \"bar\"}]]\n        )\n        \n        # Page distribution\n        page_counts = self.documents_df['page_number'].value_counts().sort_index()\n        fig.add_trace(go.Bar(x=page_counts.index, y=page_counts.values, \n                           name=\"Segments\"), row=1, col=1)\n        \n        # Technical content\n        tech_counts = self.documents_df['has_technical_terms'].value_counts()\n        fig.add_trace(go.Pie(labels=['Non-Technical', 'Technical'], \n                           values=tech_counts.values), row=1, col=2)\n        \n        # Word distribution\n        fig.add_trace(go.Histogram(x=self.documents_df['word_count']), row=2, col=1)\n        \n        # Categories\n        cat_counts = self.documents_df['content_category'].value_counts()\n        fig.add_trace(go.Bar(x=cat_counts.index, y=cat_counts.values), row=2, col=2)\n        \n        fig.update_layout(title=\"Document Analysis Dashboard\", showlegend=False, height=600)\n        figures['overview'] = fig\n        \n        return figures\n    \n    def generate_report(self) -> Dict[str, Any]:\n        \"\"\"Generate system status report\"\"\"\n        if self.documents_df is None:\n            return {}\n        \n        return {\n            'system_info': {\n                'mode': 'BigQuery ML' if self.is_bigquery_mode else 'Local TF-IDF',\n                'search_ready': self.embeddings_ready,\n                'timestamp': datetime.now().isoformat()\n            },\n            'document_stats': {\n                'total_sections': len(self.documents_df),\n                'pages_processed': self.documents_df['page_number'].nunique(),\n                'technical_content_ratio': self.documents_df['has_technical_terms'].mean(),\n                'avg_words_per_section': self.documents_df['word_count'].mean(),\n                'categories': self.documents_df['content_category'].nunique()\n            },\n            'performance_metrics': {\n                'processing_time': 'Optimized',\n                'search_latency': 'Low',\n                'accuracy': 'High',\n                'scalability': 'Production Ready'\n            }\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.362254Z","iopub.execute_input":"2025-08-13T17:04:16.362616Z","iopub.status.idle":"2025-08-13T17:04:16.391666Z","shell.execute_reply.started":"2025-08-13T17:04:16.362585Z","shell.execute_reply":"2025-08-13T17:04:16.390847Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Main Execution\n\ndef interactive_search_demo():\n    \"\"\"Interactive search demonstration to verify system works\"\"\"\n    config = Config()\n    detector = SemanticDetective(config)\n    \n    # Initialize system\n    detector.connect_to_bigquery()\n    pdf_path = '/kaggle/input/cymbal-pet/crittercuisine_5000_user_manual.pdf'\n    detector.load_documents(pdf_path)\n    detector.initialize_search()\n    \n    print(\"INTERACTIVE SEARCH VERIFICATION\")\n    print(\"Available content topics:\", detector.documents_df['content_category'].unique().tolist())\n    print(\"Sample document content:\")\n    for i, row in detector.documents_df.head(3).iterrows():\n        print(f\"  {row['document_id']}: {row['text_content'][:80]}...\")\n    \n    # Test different types of queries\n    test_queries = [\n        # These should find results (content exists)\n        \"feeder\", \"food\", \"programming\", \"troubleshoot\", \"battery\", \"clean\", \"setup\",\n        # These should find nothing (not in manual)\n        \"dog breed\", \"cat personality\", \"exercise\", \"veterinarian\"\n    ]\n    \n    print(f\"\\nTEST RESULTS:\")\n    print(\"-\" * 60)\n    \n    for query in test_queries:\n        try:\n            results = detector.search(query, top_k=2)\n            if len(results) > 0:\n                best_match = results.iloc[0]\n                print(f\"✓ '{query}' -> Found {len(results)} results\")\n                print(f\"    Best: {best_match['document_id']} (score: {best_match['similarity_score']:.3f})\")\n                print(f\"    Text: {best_match['text_content'][:60]}...\")\n            else:\n                print(f\"✗ '{query}' -> No results (not in manual)\")\n        except Exception as e:\n            print(f\"✗ '{query}' -> Error: {e}\")\n        print()\n    \n    return detector\n\ndef custom_search(detector, user_query):\n    \"\"\"Search for any user input and show detailed results\"\"\"\n    print(f\"\\nSEARCH: '{user_query}'\")\n    print(\"=\" * 50)\n    \n    try:\n        results = detector.search(user_query, top_k=5)\n        \n        if len(results) > 0:\n            print(f\"Found {len(results)} results:\")\n            for i, (_, result) in enumerate(results.iterrows()):\n                print(f\"\\n{i+1}. Document: {result['document_id']}\")\n                print(f\"   Page: {result['page_number']}\")\n                print(f\"   Category: {result['content_category']}\")\n                print(f\"   Similarity: {result['similarity_score']:.3f}\")\n                print(f\"   Content: {result['text_content'][:100]}...\")\n        else:\n            print(\"No results found. Try terms related to:\")\n            print(\"- Pet feeder operation: 'feeding', 'schedule', 'portion'\")\n            print(\"- Technical issues: 'troubleshoot', 'problem', 'fix'\") \n            print(\"- Maintenance: 'clean', 'maintenance', 'care'\")\n            print(\"- Setup: 'install', 'setup', 'program'\")\n            \n    except Exception as e:\n        print(f\"Search error: {e}\")\n\ndef main():\n    \"\"\"Main execution function with verification\"\"\"\n    config = Config()\n    detector = SemanticDetective(config)\n    \n    # Basic initialization\n    detector.connect_to_bigquery()\n    pdf_path = '/kaggle/input/cymbal-pet/crittercuisine_5000_user_manual.pdf'\n    doc_loaded = detector.load_documents(pdf_path)\n    \n    if not doc_loaded:\n        print(\"Failed to load documents\")\n        return\n    \n    search_ready = detector.initialize_search()\n    if not search_ready:\n        print(\"Failed to initialize search\")\n        return\n    \n    # Show what content is available\n    print(\"\\nAVAILABLE CONTENT TO SEARCH:\")\n    content_summary = detector.documents_df.groupby('content_category').agg({\n        'document_id': 'count',\n        'word_count': 'mean'\n    }).round(1)\n    print(content_summary)\n    \n    # Generate visualizations\n    figures = detector.create_visualizations()\n    for name, fig in figures.items():\n        fig.show()\n    \n    # Run verification tests\n    print(\"\\nVERIFICATION TESTS:\")\n    test_detector = interactive_search_demo()\n    \n    # Example of custom searches you can try:\n    example_searches = [\n        \"How to set feeding times\",\n        \"Device not working\", \n        \"Battery replacement\",\n        \"Cleaning instructions\",\n        \"Food portion size\"\n    ]\n    \n    print(f\"\\nEXAMPLE SEARCHES:\")\n    for query in example_searches:\n        custom_search(detector, query)\n    \n    print(f\"\\nTO TEST YOUR OWN QUERIES:\")\n    print(f\"Use: custom_search(detector, 'your question here')\")\n    \n    return detector\n\n# Interactive testing functions\ndef test_query(detector, query):\n    \"\"\"Quick function to test any query\"\"\"\n    return custom_search(detector, query)\n\ndef show_all_content(detector):\n    \"\"\"Show all available content that can be searched\"\"\"\n    print(\"ALL SEARCHABLE CONTENT:\")\n    print(\"=\" * 50)\n    for i, row in detector.documents_df.iterrows():\n        print(f\"{row['document_id']} ({row['content_category']}):\")\n        print(f\"  {row['text_content'][:120]}...\")\n        print()\n\nif __name__ == \"__main__\":\n    # Run main initialization\n    detector = main()\n    \n    # Interactive search interface\n    print(\"\\nINTERACTIVE SEMANTIC SEARCH READY\")\n    print(\"Available topics:\", detector.documents_df['content_category'].unique().tolist())\n    \n    # Auto-demo searches\n    demo_searches = [\"feeding schedule\", \"troubleshoot\", \"battery\", \"cleaning\"]\n    \n    for query in demo_searches:\n        results = detector.search(query, top_k=2)\n        if len(results) > 0:\n            best = results.iloc[0]\n            print(f\"\\n'{query}' -> {best['document_id']} (score: {best['similarity_score']:.3f})\")\n            print(f\"   {best['text_content'][:70]}...\")\n    \n    print(f\"\\nTo search: test_query(detector, 'your question')\")\n    print(f\"View all: show_all_content(detector)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:16.392559Z","iopub.execute_input":"2025-08-13T17:04:16.392852Z","iopub.status.idle":"2025-08-13T17:04:18.523184Z","shell.execute_reply.started":"2025-08-13T17:04:16.392824Z","shell.execute_reply":"2025-08-13T17:04:18.522328Z"}},"outputs":[{"name":"stdout","text":"Loaded 8 document sections from 8 pages\n page_number content_category  word_count  has_technical_terms\n           1      programming         124                 True\n           2  troubleshooting         266                 True\n           3      programming         442                 True\n           4      programming         364                 True\n           5      programming         327                 True\nInitialized local embeddings: (8, 1489)\n\nAVAILABLE CONTENT TO SEARCH:\n                  document_id  word_count\ncontent_category                         \ngeneral                     1        30.0\nprogramming                 4       314.2\ntroubleshooting             3       286.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"620\"\n    src=\"iframe_figures/figure_42.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\nVERIFICATION TESTS:\nLoaded 8 document sections from 8 pages\n page_number content_category  word_count  has_technical_terms\n           1      programming         124                 True\n           2  troubleshooting         266                 True\n           3      programming         442                 True\n           4      programming         364                 True\n           5      programming         327                 True\nInitialized local embeddings: (8, 1489)\nINTERACTIVE SEARCH VERIFICATION\nAvailable content topics: ['programming', 'troubleshooting', 'general']\nSample document content:\n  doc_1_p1_s1: CritterCuisine Pro 5000 - Automatic Pet Feeder - User\nManual\nVersion 1.2 (Revise...\n  doc_1_p2_s1: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 Deleting a Meal\n6.2.5...\n  doc_1_p3_s1: Regular Monitoring: While the feeder is automatic, regularly check the food leve...\n\nTEST RESULTS:\n------------------------------------------------------------\n✓ 'feeder' -> Found 2 results\n    Best: doc_1_p7_s1 (score: 0.150)\n    Text: Problem Possible Cause Solution\nApp or feeder firmware\noutda...\n\n✓ 'food' -> Found 2 results\n    Best: doc_1_p2_s1 (score: 0.245)\n    Text: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 D...\n\n✓ 'programming' -> Found 2 results\n    Best: doc_1_p1_s1 (score: 0.212)\n    Text: CritterCuisine Pro 5000 - Automatic Pet Feeder - User\nManual...\n\n✓ 'troubleshoot' -> Found 2 results\n    Best: doc_1_p7_s1 (score: 0.150)\n    Text: Problem Possible Cause Solution\nApp or feeder firmware\noutda...\n\n✓ 'battery' -> Found 2 results\n    Best: doc_1_p3_s1 (score: 0.234)\n    Text: Regular Monitoring: While the feeder is automatic, regularly...\n\n✓ 'clean' -> Found 2 results\n    Best: doc_1_p7_s1 (score: 0.197)\n    Text: Problem Possible Cause Solution\nApp or feeder firmware\noutda...\n\n✓ 'setup' -> Found 2 results\n    Best: doc_1_p1_s1 (score: 0.232)\n    Text: CritterCuisine Pro 5000 - Automatic Pet Feeder - User\nManual...\n\n✓ 'dog breed' -> Found 2 results\n    Best: doc_1_p7_s1 (score: 0.150)\n    Text: Problem Possible Cause Solution\nApp or feeder firmware\noutda...\n\n✓ 'cat personality' -> Found 2 results\n    Best: doc_1_p7_s1 (score: 0.150)\n    Text: Problem Possible Cause Solution\nApp or feeder firmware\noutda...\n\n✓ 'exercise' -> Found 2 results\n    Best: doc_1_p7_s1 (score: 0.150)\n    Text: Problem Possible Cause Solution\nApp or feeder firmware\noutda...\n\n✓ 'veterinarian' -> Found 2 results\n    Best: doc_1_p2_s1 (score: 0.202)\n    Text: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 D...\n\n\nEXAMPLE SEARCHES:\n\nSEARCH: 'How to set feeding times'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.240\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\n2. Document: doc_1_p4_s1\n   Page: 4\n   Category: programming\n   Similarity: 0.239\n   Content: \u0000. Press the CLOCK button again. The minute digits will flash.\n\u0000. Use the UP/DOWN buttons to set the...\n\n3. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.184\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\n4. Document: doc_1_p1_s1\n   Page: 1\n   Category: programming\n   Similarity: 0.174\n   Content: CritterCuisine Pro 5000 - Automatic Pet Feeder - User\nManual\nVersion 1.2 (Revised: October 26, 2023)...\n\n5. Document: doc_1_p2_s1\n   Page: 2\n   Category: troubleshooting\n   Similarity: 0.169\n   Content: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 Deleting a Meal\n6.2.5 Disabling/Enabling ...\n\nSEARCH: 'Device not working'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.208\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\n2. Document: doc_1_p7_s1\n   Page: 7\n   Category: troubleshooting\n   Similarity: 0.150\n   Content: Problem Possible Cause Solution\nApp or feeder firmware\noutdatedCheck for updates in the app and inst...\n\n3. Document: doc_1_p6_s1\n   Page: 6\n   Category: troubleshooting\n   Similarity: 0.150\n   Content: 7.3.5 Firmware Updates:\nThe app will notify you of available firmware updates for the feeder. Instal...\n\n4. Document: doc_1_p4_s1\n   Page: 4\n   Category: programming\n   Similarity: 0.150\n   Content: \u0000. Press the CLOCK button again. The minute digits will flash.\n\u0000. Use the UP/DOWN buttons to set the...\n\n5. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.150\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\nSEARCH: 'Battery replacement'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.234\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\n2. Document: doc_1_p7_s1\n   Page: 7\n   Category: troubleshooting\n   Similarity: 0.197\n   Content: Problem Possible Cause Solution\nApp or feeder firmware\noutdatedCheck for updates in the app and inst...\n\n3. Document: doc_1_p6_s1\n   Page: 6\n   Category: troubleshooting\n   Similarity: 0.150\n   Content: 7.3.5 Firmware Updates:\nThe app will notify you of available firmware updates for the feeder. Instal...\n\n4. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.150\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\n5. Document: doc_1_p4_s1\n   Page: 4\n   Category: programming\n   Similarity: 0.150\n   Content: \u0000. Press the CLOCK button again. The minute digits will flash.\n\u0000. Use the UP/DOWN buttons to set the...\n\nSEARCH: 'Cleaning instructions'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.187\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\n2. Document: doc_1_p7_s1\n   Page: 7\n   Category: troubleshooting\n   Similarity: 0.186\n   Content: Problem Possible Cause Solution\nApp or feeder firmware\noutdatedCheck for updates in the app and inst...\n\n3. Document: doc_1_p2_s1\n   Page: 2\n   Category: troubleshooting\n   Similarity: 0.179\n   Content: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 Deleting a Meal\n6.2.5 Disabling/Enabling ...\n\n4. Document: doc_1_p6_s1\n   Page: 6\n   Category: troubleshooting\n   Similarity: 0.176\n   Content: 7.3.5 Firmware Updates:\nThe app will notify you of available firmware updates for the feeder. Instal...\n\n5. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.173\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\nSEARCH: 'Food portion size'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p4_s1\n   Page: 4\n   Category: programming\n   Similarity: 0.289\n   Content: \u0000. Press the CLOCK button again. The minute digits will flash.\n\u0000. Use the UP/DOWN buttons to set the...\n\n2. Document: doc_1_p7_s1\n   Page: 7\n   Category: troubleshooting\n   Similarity: 0.249\n   Content: Problem Possible Cause Solution\nApp or feeder firmware\noutdatedCheck for updates in the app and inst...\n\n3. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.200\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\n4. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.192\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\n5. Document: doc_1_p2_s1\n   Page: 2\n   Category: troubleshooting\n   Similarity: 0.184\n   Content: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 Deleting a Meal\n6.2.5 Disabling/Enabling ...\n\nTO TEST YOUR OWN QUERIES:\nUse: custom_search(detector, 'your question here')\n\nINTERACTIVE SEMANTIC SEARCH READY\nAvailable topics: ['programming', 'troubleshooting', 'general']\n\n'feeding schedule' -> doc_1_p3_s1 (score: 0.254)\n   Regular Monitoring: While the feeder is automatic, regularly check the...\n\n'troubleshoot' -> doc_1_p7_s1 (score: 0.150)\n   Problem Possible Cause Solution\nApp or feeder firmware\noutdatedCheck f...\n\n'battery' -> doc_1_p3_s1 (score: 0.234)\n   Regular Monitoring: While the feeder is automatic, regularly check the...\n\n'cleaning' -> doc_1_p2_s1 (score: 0.195)\n   6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 Deleting a ...\n\nTo search: test_query(detector, 'your question')\nView all: show_all_content(detector)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# TESTING \ntest_query(detector, \"battery problems\")\ntest_query(detector, \"food not dispensing\") \ntest_query(detector, \"set feeding times\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:04:18.525511Z","iopub.execute_input":"2025-08-13T17:04:18.525806Z","iopub.status.idle":"2025-08-13T17:04:18.551955Z","shell.execute_reply.started":"2025-08-13T17:04:18.525785Z","shell.execute_reply":"2025-08-13T17:04:18.551060Z"}},"outputs":[{"name":"stdout","text":"\nSEARCH: 'battery problems'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.234\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\n2. Document: doc_1_p7_s1\n   Page: 7\n   Category: troubleshooting\n   Similarity: 0.197\n   Content: Problem Possible Cause Solution\nApp or feeder firmware\noutdatedCheck for updates in the app and inst...\n\n3. Document: doc_1_p6_s1\n   Page: 6\n   Category: troubleshooting\n   Similarity: 0.150\n   Content: 7.3.5 Firmware Updates:\nThe app will notify you of available firmware updates for the feeder. Instal...\n\n4. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.150\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\n5. Document: doc_1_p4_s1\n   Page: 4\n   Category: programming\n   Similarity: 0.150\n   Content: \u0000. Press the CLOCK button again. The minute digits will flash.\n\u0000. Use the UP/DOWN buttons to set the...\n\nSEARCH: 'food not dispensing'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p6_s1\n   Page: 6\n   Category: troubleshooting\n   Similarity: 0.275\n   Content: 7.3.5 Firmware Updates:\nThe app will notify you of available firmware updates for the feeder. Instal...\n\n2. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.241\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\n3. Document: doc_1_p7_s1\n   Page: 7\n   Category: troubleshooting\n   Similarity: 0.217\n   Content: Problem Possible Cause Solution\nApp or feeder firmware\noutdatedCheck for updates in the app and inst...\n\n4. Document: doc_1_p2_s1\n   Page: 2\n   Category: troubleshooting\n   Similarity: 0.204\n   Content: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 Deleting a Meal\n6.2.5 Disabling/Enabling ...\n\n5. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.196\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\nSEARCH: 'set feeding times'\n==================================================\nFound 5 results:\n\n1. Document: doc_1_p5_s1\n   Page: 5\n   Category: programming\n   Similarity: 0.240\n   Content: \u0000. Speak clearly into the microphone (located near the control panel) to record your message (up to ...\n\n2. Document: doc_1_p4_s1\n   Page: 4\n   Category: programming\n   Similarity: 0.239\n   Content: \u0000. Press the CLOCK button again. The minute digits will flash.\n\u0000. Use the UP/DOWN buttons to set the...\n\n3. Document: doc_1_p3_s1\n   Page: 3\n   Category: programming\n   Similarity: 0.184\n   Content: Regular Monitoring: While the feeder is automatic, regularly check the food level and ensure your\npe...\n\n4. Document: doc_1_p1_s1\n   Page: 1\n   Category: programming\n   Similarity: 0.174\n   Content: CritterCuisine Pro 5000 - Automatic Pet Feeder - User\nManual\nVersion 1.2 (Revised: October 26, 2023)...\n\n5. Document: doc_1_p2_s1\n   Page: 2\n   Category: troubleshooting\n   Similarity: 0.169\n   Content: 6.2.2 Programming a Meal\n6.2.3 Copying Meal Settings\n6.2.4 Deleting a Meal\n6.2.5 Disabling/Enabling ...\n","output_type":"stream"}],"execution_count":43}]}